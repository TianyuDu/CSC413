{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fr75-y9A4zcM"
   },
   "source": [
    "# Programming Assignment 1: Learning Distributed Word Representations\n",
    "Based on an assignment by George Dahl\n",
    "\n",
    "For CSC413/2516 in Winter 2020 with Professor Jimmy Ba\n",
    "\n",
    "**Submission:**\n",
    "You must submit two files through MarkUs: a PDF file containing your writeup, titled *a1-writeup.pdf*, and your code file *language_model.ipynb*. Your writeup must be typed.\n",
    "\n",
    "The programming assignments are individual work. See the Course Syllabus for detailed policies. \n",
    "\n",
    "**Introduction:**\n",
    "In this assignment we will learn about word embeddings and make neural networks learn about words.\n",
    "We could try to match statistics about the words, or we could train a network that takes a sequence of words as input and learns to predict the word that comes next.\n",
    "        \n",
    "This assignment will ask you to implement a linear embedding and then the backpropagation computations for a neural language model and then run some experiments to analyze the learned representation.\n",
    "The amount of code you have to write is very short but each line will require you to think very carefully.\n",
    "You will need to derive the updates mathematically, and then implement them using matrix and vector operations in NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UUSJPdr3Ge_"
   },
   "source": [
    "# Starter code and data\n",
    "\n",
    "Download and extract the archive from the course web page.\n",
    "Look at the file *raw_sentences.txt*.\n",
    "\n",
    "It contains the sentences that we will be using for this assignment.\n",
    "These sentences are fairly simple ones and cover a vocabulary of only 250 words.\n",
    "\n",
    "First, perform the required imports for your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRwuwhoJ3Knl"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pylab\n",
    "use_colab = False\n",
    "if use_colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "TINY = 1e-30\n",
    "EPS = 1e-4\n",
    "nax = np.newaxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNLvRXdy3NDO"
   },
   "source": [
    "If you're using colaboratory, you can create a folder in your google drive - here we used 'CSC413_A1' - with the extracted contents of *a1-code.zip*.\n",
    "Then you can mount your drive to access the data.\n",
    "Feel free to use a different way to access the files *data.pk* and *partially_trained.pk*.\n",
    "You may have to use to enter an authorization code to mount your drive if you use the google drive to store the data.\n",
    "\n",
    "If you're not using colaboratory, then set the paths to wherever the extracted contents are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gkug8am63SzY"
   },
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    drive_name = '/content/drive'\n",
    "    drive.mount(drive_name)\n",
    "    drive_413_A1_folder = 'CSC413_A1'\n",
    "    drive_location = drive_name + '/My Drive/' + drive_413_A1_folder  # Change this to where your files are located\n",
    "else:\n",
    "    # set the drive_location variable to whereever the extracted contents are.\n",
    "    drive_location = '/'\n",
    "\n",
    "data_location = drive_location + '/' + 'data.pk'\n",
    "PARTIALLY_TRAINED_MODEL = drive_location + '/' + 'partially_trained.pk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qna9z_wJ3U5e"
   },
   "source": [
    "We have already extracted the 4-grams from this dataset and divided them into training, validation, and test sets.\n",
    "To inspect this data, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RD1LN16d3a0u"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-673035ceaa72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data.pk'"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open(data_location, 'rb'))\n",
    "print(data['vocab'][0])\n",
    "print(data['train_inputs'][:10])\n",
    "print(data['train_targets'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXd2Msqs3fPQ"
   },
   "source": [
    "Now *data* is a Python dict which contains the vocabulary, as well as the inputs and targets for all three splits of the data. *data*['vocab'] is a list of the 250 words in the dictionary; *data*['vocab'][0] is the word with index 0, and so on. *data*['train_inputs'] is a 372,500 x 3 matrix where each row gives the indices of the 3 context words for one of the 372,500 training cases.\n",
    "*data*['train_targets'] is a vector giving the index of the target word for each training case. The validation and test sets are handled analogously.\n",
    "\n",
    "Even though you only have to modify two specific locations in the code, you may want to read through this code before starting the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pa9ggqxJPPs0"
   },
   "source": [
    "# Part 1: GLoVE Word Representations (4pts)\n",
    "\n",
    "In this part of the assignment, you will implement a simplified version of the GLoVE embedding (please see the handout for detailed description of the algorithm) with the loss defined as\n",
    "\n",
    "$L(\\{\\mathbf{w}_i,b_i\\}_{i=1}^V) = \\sum_{i,j=1}^V (\\mathbf{w}_i^\\top\\mathbf{w}_j + b_i + b_j - \\log X_{ij})^2$.\n",
    "\n",
    "Note that each word is represented by a d-dimensional vector $\\mathbf{w}_i$ and a scalar bias $b_i$.\n",
    "\n",
    "We have provided a few functions for training the embedding:\n",
    "\n",
    "*   *calculate_log_co_occurence* computes the log co-occurrence matrix of a given corpus\n",
    "*   *train_GLoVE* runs momentum gradient descent to optimize the embedding\n",
    "*   *loss_GLoVE:* INPUT - $V\\times d$ matrix $W$ (collection of $V$ embedding vectors, each d-dimensional); $V\\times 1$ vector $\\mathbf{b}$ (collection of $V$ bias terms); $V \\times V$ log co-occurrence matrix. OUTPUT - loss of the GLoVE objective\n",
    "*   *grad_GLoVE:* INPUT - $V\\times d$ matrix $W$, $V\\times 1$ vector b, and $V\\times V$ log co-occurrence matrix. OUTPUT - $V\\times d$ matrix grad_W containing the gradient of the loss function w.r.t. $W$; $V\\times 1$ vector grad_b which is the gradient of the loss function w.r.t. $\\mathbf{b}$. TO BE IMPLEMENTED.\n",
    "\n",
    "Run the code to compute the co-occurence matrix.\n",
    "Make sure to add a 1 to the occurences, so there are no 0's in the matrix when we take the elementwise log of the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbYUnHGQNbgu"
   },
   "outputs": [],
   "source": [
    "vocab_size = 250\n",
    "\n",
    "def calculate_log_co_occurence(word_data):\n",
    "  \"Compute the log-co-occurence matrix for our data.\"\n",
    "  log_co_occurence = np.zeros((vocab_size, vocab_size))\n",
    "  for input in word_data:\n",
    "    log_co_occurence[input[0], input[1]] += 1\n",
    "    log_co_occurence[input[1], input[2]] += 1\n",
    "    # If we want symmetric co-occurence can also increment for these.\n",
    "    # Optional: How would you generalize the model if our target co-occurence isn't symmetric?\n",
    "    log_co_occurence[input[1], input[0]] += 1\n",
    "    log_co_occurence[input[2], input[1]] += 1\n",
    "  delta_smoothing = 0.5  # A hyperparameter.  You can play with this if you want.\n",
    "  log_co_occurence += delta_smoothing  # Add delta so log doesn't break on 0's.\n",
    "  log_co_occurence = np.log(log_co_occurence)\n",
    "  return log_co_occurence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsIN6RrwONPf"
   },
   "outputs": [],
   "source": [
    "log_co_occurence_train = calculate_log_co_occurence(data['train_inputs'])\n",
    "log_co_occurence_valid = calculate_log_co_occurence(data['valid_inputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNnKkMy-d2bB"
   },
   "source": [
    "TO BE IMPLEMENTED: calculate the gradient of the loss function w.r.t. the parameters $W$ and $\\mathbf{b}$. You should vectorize the computation, i.e. not loop over every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbpkXeaAdwnj"
   },
   "outputs": [],
   "source": [
    "def loss_GLoVE(W, b, log_co_occurence):\n",
    "  \"Compute the GLoVE loss.\"\n",
    "  n,_ = log_co_occurence.shape\n",
    "  return np.sum((W @ W.T + b @ np.ones([1,n]) + np.ones([n,1])@b.T - log_co_occurence)**2)\n",
    "\n",
    "def grad_GLoVE(W,  b, log_co_occurence):\n",
    "  \"Return the gradient of GLoVE objective w.r.t W and b.\"\n",
    "  \"INPUT: W - Vxd; b - Vx1; log_co_occurence: VxV\"\n",
    "  \"OUTPUT: grad_W - Vxd; grad_b - Vx1\"\n",
    "  n,_ = log_co_occurence.shape\n",
    "  ###########################   YOUR CODE HERE  ##############################\n",
    "\n",
    "  \n",
    "  ############################################################################\n",
    "  return grad_W, grad_b\n",
    "\n",
    "def train_GLoVE(W, b, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=False):\n",
    "  \"Traing W and b according to GLoVE objective.\"\n",
    "  n,_ = log_co_occurence_train.shape\n",
    "  learning_rate = 0.5 / n  # A hyperparameter.  You can play with this if you want.\n",
    "  for epoch in range(n_epochs):\n",
    "    grad_W, grad_b = grad_GLoVE(W, b, log_co_occurence_train)\n",
    "    W -= learning_rate * grad_W\n",
    "    b -= learning_rate * grad_b\n",
    "    train_loss, valid_loss = loss_GLoVE(W, b, log_co_occurence_train), loss_GLoVE(W, b, log_co_occurence_valid)\n",
    "    if do_print:\n",
    "      print(f\"Train Loss: {train_loss}, valid loss: {valid_loss}, grad_norm: {np.sum(grad_w**2)}\")\n",
    "  return W, b, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjiNQ0WkWi1Z"
   },
   "source": [
    "Train the GLoVE model for a range of embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46yGUezEMLJe"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n_epochs = 500  # A hyperparameter.  You can play with this if you want.\n",
    "embedding_dims = np.array([1,3,5,7,10,12,15,20,25,30,40,50])  # Play with this\n",
    "final_train_losses, final_val_losses = [], []  # Store the final losses for graphing\n",
    "W_final_2d, b_final_2d = None, None\n",
    "do_print = False  # If you want to see diagnostic information during training\n",
    "for embedding_dim in tqdm(embedding_dims):\n",
    "  init_variance = 0.1  # A hyperparameter.  You can play with this if you want.\n",
    "  W = init_variance * np.random.normal(size=(250, embedding_dim))\n",
    "  b = init_variance * np.random.normal(size=(250, 1))\n",
    "  if do_print:\n",
    "    print(f\"Training for embedding dimension: {embedding_dim}\")\n",
    "  W_final, b_final, train_loss, valid_loss = train_GLoVE(W, b, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=do_print)\n",
    "  if embedding_dim == 2:\n",
    "    # Save a parameter copy if we are training 2d embedding for visualization later\n",
    "    W_final_2d = W_final\n",
    "    b_final_2d = b_final\n",
    "  final_train_losses += [train_loss]\n",
    "  final_val_losses += [valid_loss]\n",
    "  if do_print:\n",
    "    print(f\"Final validation loss: {valid_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzV-qFf5WfAp"
   },
   "source": [
    "Plot the training and validation losses against the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHgHgSzJTg5d"
   },
   "outputs": [],
   "source": [
    "pylab.loglog(embedding_dims, final_train_losses)\n",
    "pylab.xlabel(\"Embedding Dimension\")\n",
    "pylab.ylabel(\"Training Loss\")\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UJSfg_hfvIV"
   },
   "outputs": [],
   "source": [
    "pylab.loglog(embedding_dims, final_val_losses)\n",
    "pylab.xlabel(\"Embedding Dimension\")\n",
    "pylab.ylabel(\"Validation Loss\")\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xo1R6rfP4aJQ"
   },
   "source": [
    "Answer the following questions:\n",
    "\n",
    "1.  Given the vocabulary size $V$ and embedding dimensionality $d$, how many parameters does the GLoVE model have?\n",
    "2.  Write the gradient of the loss function with respect to one parameter vector $\\mathbf{w}_i$.\n",
    "3.  Implement the gradient update of GLoVE.\n",
    "4.  Train the model with varying dimensionality $d$.\n",
    "Which $d$ leads to optimal validation performance?\n",
    "Why does / doesn't larger $d$ always lead to better validation error?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YwDZcOhjywe"
   },
   "source": [
    "# Part 2: Network Architecture (2pts)\n",
    "See the handout for the written questions in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8FZTyrzlCNl"
   },
   "source": [
    "# Part 3: Training the model (4pts)\n",
    "\n",
    "There are three classes defined in this *part*: *Params*, *Activations*, *Model*.\n",
    "You will make changes to *Model*, but it may help to read through *Params* and *Activations* first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfGEjB3QLNXf"
   },
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "    \"\"\"A class representing the trainable parameters of the model. This class has five fields:\n",
    "    \n",
    "           word_embedding_weights, a matrix of size N_V x D, where N_V is the number of words in the vocabulary\n",
    "                   and D is the embedding dimension.\n",
    "           embed_to_hid_weights, a matrix of size N_H x 3D, where N_H is the number of hidden units. The first D\n",
    "                   columns represent connections from the embedding of the first context word, the next D columns\n",
    "                   for the second context word, and so on.\n",
    "           hid_bias, a vector of length N_H\n",
    "           hid_to_output_weights, a matrix of size N_V x N_H\n",
    "           output_bias, a vector of length N_V\"\"\"\n",
    "\n",
    "    def __init__(self, word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
    "                 hid_bias, output_bias):\n",
    "        self.word_embedding_weights = word_embedding_weights\n",
    "        self.embed_to_hid_weights = embed_to_hid_weights\n",
    "        self.hid_to_output_weights = hid_to_output_weights\n",
    "        self.hid_bias = hid_bias\n",
    "        self.output_bias = output_bias\n",
    "\n",
    "    def copy(self):\n",
    "        return self.__class__(self.word_embedding_weights.copy(), self.embed_to_hid_weights.copy(),\n",
    "                              self.hid_to_output_weights.copy(), self.hid_bias.copy(), self.output_bias.copy())\n",
    "\n",
    "    @classmethod\n",
    "    def zeros(cls, vocab_size, context_len, embedding_dim, num_hid):\n",
    "        \"\"\"A constructor which initializes all weights and biases to 0.\"\"\"\n",
    "        word_embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
    "        embed_to_hid_weights = np.zeros((num_hid, context_len * embedding_dim))\n",
    "        hid_to_output_weights = np.zeros((vocab_size, num_hid))\n",
    "        hid_bias = np.zeros(num_hid)\n",
    "        output_bias = np.zeros(vocab_size)\n",
    "        return cls(word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
    "                   hid_bias, output_bias)\n",
    "\n",
    "    @classmethod\n",
    "    def random_init(cls, init_wt, vocab_size, context_len, embedding_dim, num_hid):\n",
    "        \"\"\"A constructor which initializes weights to small random values and biases to 0.\"\"\"\n",
    "        word_embedding_weights = np.random.normal(0., init_wt, size=(vocab_size, embedding_dim))\n",
    "        embed_to_hid_weights = np.random.normal(0., init_wt, size=(num_hid, context_len * embedding_dim))\n",
    "        hid_to_output_weights = np.random.normal(0., init_wt, size=(vocab_size, num_hid))\n",
    "        hid_bias = np.zeros(num_hid)\n",
    "        output_bias = np.zeros(vocab_size)\n",
    "        return cls(word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
    "                   hid_bias, output_bias)\n",
    "\n",
    "    ###### The functions below are Python's somewhat oddball way of overloading operators, so that\n",
    "    ###### we can do arithmetic on Params instances. You don't need to understand this to do the assignment.\n",
    "\n",
    "    def __mul__(self, a):\n",
    "        return self.__class__(a * self.word_embedding_weights,\n",
    "                              a * self.embed_to_hid_weights,\n",
    "                              a * self.hid_to_output_weights,\n",
    "                              a * self.hid_bias,\n",
    "                              a * self.output_bias)\n",
    "\n",
    "    def __rmul__(self, a):\n",
    "        return self * a\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return self.__class__(self.word_embedding_weights + other.word_embedding_weights,\n",
    "                              self.embed_to_hid_weights + other.embed_to_hid_weights,\n",
    "                              self.hid_to_output_weights + other.hid_to_output_weights,\n",
    "                              self.hid_bias + other.hid_bias,\n",
    "                              self.output_bias + other.output_bias)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + -1. * other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6XFQUPsLSi7"
   },
   "outputs": [],
   "source": [
    "class Activations(object):\n",
    "    \"\"\"A class representing the activations of the units in the network. This class has three fields:\n",
    "\n",
    "        embedding_layer, a matrix of B x 3D matrix (where B is the batch size and D is the embedding dimension),\n",
    "                representing the activations for the embedding layer on all the cases in a batch. The first D\n",
    "                columns represent the embeddings for the first context word, and so on.\n",
    "        hidden_layer, a B x N_H matrix representing the hidden layer activations for a batch\n",
    "        output_layer, a B x N_V matrix representing the output layer activations for a batch\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_layer, hidden_layer, output_layer):\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "\n",
    "def get_batches(inputs, targets, batch_size, shuffle=True):\n",
    "    \"\"\"Divide a dataset (usually the training set) into mini-batches of a given size. This is a\n",
    "    'generator', i.e. something you can use in a for loop. You don't need to understand how it\n",
    "    works to do the assignment.\"\"\"\n",
    "\n",
    "    if inputs.shape[0] % batch_size != 0:\n",
    "        raise RuntimeError('The number of data points must be a multiple of the batch size.')\n",
    "    num_batches = inputs.shape[0] // batch_size\n",
    "\n",
    "    if shuffle:\n",
    "        idxs = np.random.permutation(inputs.shape[0])\n",
    "        inputs = inputs[idxs, :]\n",
    "        targets = targets[idxs]\n",
    "\n",
    "    for m in range(num_batches):\n",
    "        yield inputs[m * batch_size:(m + 1) * batch_size, :], \\\n",
    "              targets[m * batch_size:(m + 1) * batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uuAXaDNll0lf"
   },
   "source": [
    "In this part of the assignment, you implement a method which computes the gradient using backpropagation.\n",
    "To start you out, the *Model* class contains several important methods used in training:\n",
    "\n",
    "\n",
    "*   *compute_activations* computes the activations of all units on a given input batch\n",
    "*   *compute_loss* computes the total cross-entropy loss on a mini-batch\n",
    "*   *evaluate* computes the average cross-entropy loss for a given set of inputs and targets\n",
    "\n",
    "You will need to complete the implementation of two additional methods which are needed for training:\n",
    "\n",
    "\n",
    "*   *compute_loss_derivative* computes the derivative of the loss function with respect to the output layer inputs.\n",
    "*   *back_propagate* is the function which computes the gradient of the loss with respect to model parameters using backpropagation.\n",
    "It uses the derivatives computed by *compute_loss_derivative*.\n",
    "Some parts are already filled in for you, but you need to compute the matrices of derivatives for *embed_to_hid_weights*, *hid_bias*, *hid_to_output_weights*, and *output_bias*.\n",
    "These matrices have the same sizes as the parameter matrices (see previous section).\n",
    "\n",
    "In order to implement backpropagation efficiently, you need to express the computations in terms of matrix operations, rather than *for* loops.\n",
    "You should first work through the derivatives on pencil and paper.\n",
    "First, apply the chain rule to compute the derivatives with respect to individual units, weights, and biases.\n",
    "Next, take the formulas you've derived, and express them in matrix form.\n",
    "You should be able to express all of the required computations using only matrix multiplication, matrix transpose, and elementwise operations --- no *for* loops!\n",
    "If you want inspiration, read through the code for *Model.compute_activations* and try to understand how the matrix operations correspond to the computations performed by all the units in the network.\n",
    "        \n",
    "To make your life easier, we have provided the routine *checking.check_gradients*, which checks your gradients using finite differences.\n",
    "You should make sure this check passes before continuing with the assignment.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0F4CTBipK9B6"
   },
   "outputs": [],
   "source": [
    "     class Model(object):\n",
    "    \"\"\"A class representing the language model itself. This class contains various methods used in training\n",
    "    the model and visualizing the learned representations. It has two fields:\n",
    "\n",
    "        params, a Params instance which contains the model parameters\n",
    "        vocab, a list containing all the words in the dictionary; vocab[0] is the word with index\n",
    "               0, and so on.\"\"\"\n",
    "\n",
    "    def __init__(self, params, vocab):\n",
    "        self.params = params\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.embedding_dim = self.params.word_embedding_weights.shape[1]\n",
    "        self.embedding_layer_dim = self.params.embed_to_hid_weights.shape[1]\n",
    "        self.context_len = self.embedding_layer_dim // self.embedding_dim\n",
    "        self.num_hid = self.params.embed_to_hid_weights.shape[0]\n",
    "\n",
    "    def copy(self):\n",
    "        return self.__class__(self.params.copy(), self.vocab[:])\n",
    "\n",
    "    @classmethod\n",
    "    def random_init(cls, init_wt, vocab, context_len, embedding_dim, num_hid):\n",
    "        \"\"\"Constructor which randomly initializes the weights to Gaussians with standard deviation init_wt\n",
    "        and initializes the biases to all zeros.\"\"\"\n",
    "        params = Params.random_init(init_wt, len(vocab), context_len, embedding_dim, num_hid)\n",
    "        return Model(params, vocab)\n",
    "\n",
    "    def indicator_matrix(self, targets):\n",
    "        \"\"\"Construct a matrix where the kth entry of row i is 1 if the target for example\n",
    "        i is k, and all other entries are 0.\"\"\"\n",
    "        batch_size = targets.size\n",
    "        expanded_targets = np.zeros((batch_size, len(self.vocab)))\n",
    "        expanded_targets[np.arange(batch_size), targets] = 1.\n",
    "        return expanded_targets\n",
    "\n",
    "    def compute_loss_derivative(self, output_activations, expanded_target_batch):\n",
    "        \"\"\"Compute the derivative of the cross-entropy loss function with respect to the inputs\n",
    "        to the output units. In particular, the output layer computes the softmax\n",
    "\n",
    "            y_i = e^{z_i} / \\sum_j e^{z_j}.\n",
    "\n",
    "        This function should return a batch_size x vocab_size matrix, where the (i, j) entry\n",
    "        is dC / dz_j computed for the ith training case, where C is the loss function\n",
    "\n",
    "            C = -sum(t_i log y_i).\n",
    "\n",
    "        The arguments are as follows:\n",
    "\n",
    "            output_activations - the activations of the output layer, i.e. the y_i's.\n",
    "            expanded_target_batch - each row is the indicator vector for a target word,\n",
    "                i.e. the (i, j) entry is 1 if the i'th word is j, and 0 otherwise.\"\"\"\n",
    "\n",
    "        ###########################   YOUR CODE HERE  ##############################\n",
    "        return None\n",
    "        ############################################################################\n",
    "\n",
    "    def compute_loss(self, output_activations, expanded_target_batch):\n",
    "        \"\"\"Compute the total loss over a mini-batch. expanded_target_batch is the matrix obtained\n",
    "        by calling indicator_matrix on the targets for the batch.\"\"\"\n",
    "        return -np.sum(expanded_target_batch * np.log(output_activations + TINY))\n",
    "\n",
    "    def compute_activations(self, inputs):\n",
    "        \"\"\"Compute the activations on a batch given the inputs. Returns an Activations instance.\n",
    "        You should try to read and understand this function, since this will give you clues for\n",
    "        how to implement back_propagate.\"\"\"\n",
    "\n",
    "        batch_size = inputs.shape[0]\n",
    "        if inputs.shape[1] != self.context_len:\n",
    "            raise RuntimeError('Dimension of the input vectors should be {}, but is instead {}'.format(\n",
    "                self.context_len, inputs.shape[1]))\n",
    "\n",
    "        # Embedding layer\n",
    "        # Look up the input word indies in the word_embedding_weights matrix\n",
    "        embedding_layer_state = np.zeros((batch_size, self.embedding_layer_dim))\n",
    "        for i in range(self.context_len):\n",
    "            embedding_layer_state[:, i * self.embedding_dim:(i + 1) * self.embedding_dim] = \\\n",
    "                self.params.word_embedding_weights[inputs[:, i], :]\n",
    "\n",
    "        # Hidden layer\n",
    "        inputs_to_hid = np.dot(embedding_layer_state, self.params.embed_to_hid_weights.T) + \\\n",
    "                        self.params.hid_bias\n",
    "        # Apply logistic activation function\n",
    "        hidden_layer_state = 1. / (1. + np.exp(-inputs_to_hid))\n",
    "\n",
    "        # Output layer\n",
    "        inputs_to_softmax = np.dot(hidden_layer_state, self.params.hid_to_output_weights.T) + \\\n",
    "                            self.params.output_bias\n",
    "\n",
    "        # Subtract maximum.\n",
    "        # Remember that adding or subtracting the same constant from each input to a\n",
    "        # softmax unit does not affect the outputs. So subtract the maximum to\n",
    "        # make all inputs <= 0. This prevents overflows when computing their exponents.\n",
    "        inputs_to_softmax -= inputs_to_softmax.max(1).reshape((-1, 1))\n",
    "\n",
    "        output_layer_state = np.exp(inputs_to_softmax)\n",
    "        output_layer_state /= output_layer_state.sum(1).reshape((-1, 1))\n",
    "\n",
    "        return Activations(embedding_layer_state, hidden_layer_state, output_layer_state)\n",
    "\n",
    "    def back_propagate(self, input_batch, activations, loss_derivative):\n",
    "        \"\"\"Compute the gradient of the loss function with respect to the trainable parameters\n",
    "        of the model. The arguments are as follows:\n",
    "\n",
    "             input_batch - the indices of the context words\n",
    "             activations - an Activations class representing the output of Model.compute_activations\n",
    "             loss_derivative - the matrix of derivatives computed by compute_loss_derivative\n",
    "             \n",
    "        Part of this function is already completed, but you need to fill in the derivative\n",
    "        computations for hid_to_output_weights_grad, output_bias_grad, embed_to_hid_weights_grad,\n",
    "        and hid_bias_grad. See the documentation for the Params class for a description of what\n",
    "        these matrices represent.\"\"\"\n",
    "\n",
    "        # The matrix with values dC / dz_j, where dz_j is the input to the jth hidden unit,\n",
    "        # i.e. y_j = 1 / (1 + e^{-z_j})\n",
    "        hid_deriv = np.dot(loss_derivative, self.params.hid_to_output_weights) \\\n",
    "                    * activations.hidden_layer * (1. - activations.hidden_layer)\n",
    "\n",
    "        ###########################   YOUR CODE HERE  ##############################\n",
    "        ############################################################################\n",
    "\n",
    "        # The matrix of derivatives for the embedding layer\n",
    "        embed_deriv = np.dot(hid_deriv, self.params.embed_to_hid_weights)\n",
    "\n",
    "        # Embedding layer\n",
    "        word_embedding_weights_grad = np.zeros((self.vocab_size, self.embedding_dim))\n",
    "        for w in range(self.context_len):\n",
    "            word_embedding_weights_grad += np.dot(self.indicator_matrix(input_batch[:, w]).T,\n",
    "                                                  embed_deriv[:, w * self.embedding_dim:(w + 1) * self.embedding_dim])\n",
    "\n",
    "        return Params(word_embedding_weights_grad, embed_to_hid_weights_grad, hid_to_output_weights_grad,\n",
    "                      hid_bias_grad, output_bias_grad)\n",
    "\n",
    "    def evaluate(self, inputs, targets, batch_size=100):\n",
    "        \"\"\"Compute the average cross-entropy over a dataset.\n",
    "\n",
    "            inputs: matrix of shape D x N\n",
    "            targets: one-dimensional matrix of length N\"\"\"\n",
    "\n",
    "        ndata = inputs.shape[0]\n",
    "\n",
    "        total = 0.\n",
    "        for input_batch, target_batch in get_batches(inputs, targets, batch_size):\n",
    "            activations = self.compute_activations(input_batch)\n",
    "            expanded_target_batch = self.indicator_matrix(target_batch)\n",
    "            cross_entropy = -np.sum(expanded_target_batch * np.log(activations.output_layer + TINY))\n",
    "            total += cross_entropy\n",
    "\n",
    "        return total / float(ndata)\n",
    "\n",
    "    def display_nearest_words(self, word, k=10):\n",
    "        \"\"\"List the k words nearest to a given word, along with their distances.\"\"\"\n",
    "\n",
    "        if word not in self.vocab:\n",
    "            print('Word \"{}\" not in vocabulary.'.format(word))\n",
    "            return\n",
    "\n",
    "        # Compute distance to every other word.\n",
    "        idx = self.vocab.index(word)\n",
    "        word_rep = self.params.word_embedding_weights[idx, :]\n",
    "        diff = self.params.word_embedding_weights - word_rep.reshape((1, -1))\n",
    "        distance = np.sqrt(np.sum(diff ** 2, axis=1))\n",
    "\n",
    "        # Sort by distance.\n",
    "        order = np.argsort(distance)\n",
    "        order = order[1:1 + k]  # The nearest word is the query word itself, skip that.\n",
    "        for i in order:\n",
    "            print('{}: {}'.format(self.vocab[i], distance[i]))\n",
    "\n",
    "    def predict_next_word(self, word1, word2, word3, k=10):\n",
    "        \"\"\"List the top k predictions for the next word along with their probabilities.\n",
    "        Inputs:\n",
    "            word1: The first word as a string.\n",
    "            word2: The second word as a string.\n",
    "            word3: The third word as a string.\n",
    "            k: The k most probable predictions are shown.\n",
    "        Example usage:\n",
    "            model.predict_next_word('john', 'might', 'be', 3)\n",
    "            model.predict_next_word('life', 'in', 'new', 3)\"\"\"\n",
    "\n",
    "        if word1 not in self.vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n",
    "        if word2 not in self.vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n",
    "        if word3 not in self.vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word3))\n",
    "\n",
    "        idx1, idx2, idx3 = self.vocab.index(word1), self.vocab.index(word2), self.vocab.index(word3)\n",
    "        input = np.array([idx1, idx2, idx3]).reshape((1, -1))\n",
    "        activations = self.compute_activations(input)\n",
    "        prob = activations.output_layer.ravel()\n",
    "        idxs = np.argsort(prob)[::-1]  # sort descending\n",
    "        for i in idxs[:k]:\n",
    "            print('{} {} {} {} Prob: {:1.5f}'.format(word1, word2, word3, self.vocab[i], prob[i]))\n",
    "\n",
    "    def word_distance(self, word1, word2):\n",
    "        \"\"\"Compute the distance between the vector representations of two words.\"\"\"\n",
    "\n",
    "        if word1 not in self.vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n",
    "        if word2 not in self.vocab:\n",
    "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n",
    "\n",
    "        idx1, idx2 = self.vocab.index(word1), self.vocab.index(word2)\n",
    "        word_rep1 = self.params.word_embedding_weights[idx1, :]\n",
    "        word_rep2 = self.params.word_embedding_weights[idx2, :]\n",
    "        diff = word_rep1 - word_rep2\n",
    "        return np.sqrt(np.sum(diff ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbwZCTkboEhz"
   },
   "source": [
    "To make your life easier, we have provided the routine *checking.check_gradients*, which checks your gradients using finite differences.\n",
    "You should make sure this check passes before continuing with the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5soRTiRn6W4"
   },
   "outputs": [],
   "source": [
    "def relative_error(a, b):\n",
    "    return np.abs(a - b) / (np.abs(a) + np.abs(b))\n",
    "\n",
    "\n",
    "def check_output_derivatives(model, input_batch, target_batch):\n",
    "    def softmax(z):\n",
    "        z = z.copy()\n",
    "        z -= z.max(1).reshape((-1, 1))\n",
    "        y = np.exp(z)\n",
    "        y /= y.sum(1).reshape((-1, 1))\n",
    "        return y\n",
    "\n",
    "    batch_size = input_batch.shape[0]\n",
    "    z = np.random.normal(size=(batch_size, model.vocab_size))\n",
    "    y = softmax(z)\n",
    "\n",
    "    expanded_target_batch = model.indicator_matrix(target_batch)\n",
    "    loss_derivative = model.compute_loss_derivative(y, expanded_target_batch)\n",
    "\n",
    "    if loss_derivative is None:\n",
    "        print('Loss derivative not implemented yet.')\n",
    "        return False\n",
    "\n",
    "    if loss_derivative.shape != (batch_size, model.vocab_size):\n",
    "        print('Loss derivative should be size {} but is actually {}.'.format(\n",
    "            (batch_size, model.vocab_size), loss_derivative.shape))\n",
    "        return False\n",
    "\n",
    "    def obj(z):\n",
    "        y = softmax(z)\n",
    "        return model.compute_loss(y, expanded_target_batch)\n",
    "\n",
    "    for count in range(1000):\n",
    "        i, j = np.random.randint(0, loss_derivative.shape[0]), np.random.randint(0, loss_derivative.shape[1])\n",
    "\n",
    "        z_plus = z.copy()\n",
    "        z_plus[i, j] += EPS\n",
    "        obj_plus = obj(z_plus)\n",
    "\n",
    "        z_minus = z.copy()\n",
    "        z_minus[i, j] -= EPS\n",
    "        obj_minus = obj(z_minus)\n",
    "\n",
    "        empirical = (obj_plus - obj_minus) / (2. * EPS)\n",
    "        rel = relative_error(empirical, loss_derivative[i, j])\n",
    "        if rel > 1e-4:\n",
    "            print('The loss derivative has a relative error of {}, which is too large.'.format(rel))\n",
    "            return False\n",
    "\n",
    "    print('The loss derivative looks OK.')\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_param_gradient(model, param_name, input_batch, target_batch):\n",
    "    activations = model.compute_activations(input_batch)\n",
    "    expanded_target_batch = model.indicator_matrix(target_batch)\n",
    "    loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch)\n",
    "    param_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n",
    "\n",
    "    def obj(model):\n",
    "        activations = model.compute_activations(input_batch)\n",
    "        return model.compute_loss(activations.output_layer, expanded_target_batch)\n",
    "\n",
    "    dims = getattr(model.params, param_name).shape\n",
    "    is_matrix = (len(dims) == 2)\n",
    "\n",
    "    if getattr(param_gradient, param_name).shape != dims:\n",
    "        print('The gradient for {} should be size {} but is actually {}.'.format(\n",
    "            param_name, dims, getattr(param_gradient, param_name).shape))\n",
    "        return\n",
    "\n",
    "    for count in range(1000):\n",
    "        if is_matrix:\n",
    "            slc = np.random.randint(0, dims[0]), np.random.randint(0, dims[1])\n",
    "        else:\n",
    "            slc = np.random.randint(dims[0])\n",
    "\n",
    "        model_plus = model.copy()\n",
    "        getattr(model_plus.params, param_name)[slc] += EPS\n",
    "        obj_plus = obj(model_plus)\n",
    "\n",
    "        model_minus = model.copy()\n",
    "        getattr(model_minus.params, param_name)[slc] -= EPS\n",
    "        obj_minus = obj(model_minus)\n",
    "\n",
    "        empirical = (obj_plus - obj_minus) / (2. * EPS)\n",
    "        exact = getattr(param_gradient, param_name)[slc]\n",
    "        rel = relative_error(empirical, exact)\n",
    "        if rel > 1e-4:\n",
    "            print('The loss derivative has a relative error of {}, which is too large.'.format(rel))\n",
    "            return False\n",
    "\n",
    "    print('The gradient for {} looks OK.'.format(param_name))\n",
    "\n",
    "\n",
    "def load_partially_trained_model():\n",
    "    obj = pickle.load(open(PARTIALLY_TRAINED_MODEL, 'rb'))\n",
    "    params = Params(obj['word_embedding_weights'], obj['embed_to_hid_weights'],\n",
    "                                   obj['hid_to_output_weights'], obj['hid_bias'],\n",
    "                                   obj['output_bias'])\n",
    "    vocab = obj['vocab']\n",
    "    return Model(params, vocab)\n",
    "\n",
    "\n",
    "def check_gradients():\n",
    "    \"\"\"Check the computed gradients using finite differences.\"\"\"\n",
    "    np.random.seed(0)\n",
    "\n",
    "    np.seterr(all='ignore')  # suppress a warning which is harmless\n",
    "\n",
    "    model = load_partially_trained_model()\n",
    "    data_obj = pickle.load(open(data_location, 'rb'))\n",
    "    train_inputs, train_targets = data_obj['train_inputs'], data_obj['train_targets']\n",
    "    input_batch = train_inputs[:100, :]\n",
    "    target_batch = train_targets[:100]\n",
    "\n",
    "    if not check_output_derivatives(model, input_batch, target_batch):\n",
    "        return\n",
    "\n",
    "    for param_name in ['word_embedding_weights', 'embed_to_hid_weights', 'hid_to_output_weights',\n",
    "                       'hid_bias', 'output_bias']:\n",
    "        check_param_gradient(model, param_name, input_batch, target_batch)\n",
    "\n",
    "\n",
    "def print_gradients():\n",
    "    \"\"\"Print out certain derivatives for grading.\"\"\"\n",
    "\n",
    "    model = load_partially_trained_model()\n",
    "    data_obj = pickle.load(open(data_location, 'rb'))\n",
    "    train_inputs, train_targets = data_obj['train_inputs'], data_obj['train_targets']\n",
    "    input_batch = train_inputs[:100, :]\n",
    "    target_batch = train_targets[:100]\n",
    "\n",
    "    activations = model.compute_activations(input_batch)\n",
    "    expanded_target_batch = model.indicator_matrix(target_batch)\n",
    "    loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch)\n",
    "    param_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n",
    "\n",
    "    print('loss_derivative[2, 5]', loss_derivative[2, 5])\n",
    "    print('loss_derivative[2, 121]', loss_derivative[2, 121])\n",
    "    print('loss_derivative[5, 33]', loss_derivative[5, 33])\n",
    "    print('loss_derivative[5, 31]', loss_derivative[5, 31])\n",
    "    print()\n",
    "    print('param_gradient.word_embedding_weights[27, 2]', param_gradient.word_embedding_weights[27, 2])\n",
    "    print('param_gradient.word_embedding_weights[43, 3]', param_gradient.word_embedding_weights[43, 3])\n",
    "    print('param_gradient.word_embedding_weights[22, 4]', param_gradient.word_embedding_weights[22, 4])\n",
    "    print('param_gradient.word_embedding_weights[2, 5]', param_gradient.word_embedding_weights[2, 5])\n",
    "    print()\n",
    "    print('param_gradient.embed_to_hid_weights[10, 2]', param_gradient.embed_to_hid_weights[10, 2])\n",
    "    print('param_gradient.embed_to_hid_weights[15, 3]', param_gradient.embed_to_hid_weights[15, 3])\n",
    "    print('param_gradient.embed_to_hid_weights[30, 9]', param_gradient.embed_to_hid_weights[30, 9])\n",
    "    print('param_gradient.embed_to_hid_weights[35, 21]', param_gradient.embed_to_hid_weights[35, 21])\n",
    "    print()\n",
    "    print('param_gradient.hid_bias[10]', param_gradient.hid_bias[10])\n",
    "    print('param_gradient.hid_bias[20]', param_gradient.hid_bias[20])\n",
    "    print()\n",
    "    print('param_gradient.output_bias[0]', param_gradient.output_bias[0])\n",
    "    print('param_gradient.output_bias[1]', param_gradient.output_bias[1])\n",
    "    print('param_gradient.output_bias[2]', param_gradient.output_bias[2])\n",
    "    print('param_gradient.output_bias[3]', param_gradient.output_bias[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TCLl7v189SI"
   },
   "outputs": [],
   "source": [
    "#check_gradients()\n",
    "print_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtC-br-N5xGT"
   },
   "source": [
    "Once you've implemented the gradient computation, you'll need to train the model.\n",
    "The function *train* implements the main training procedure.\n",
    "It takes two arguments:\n",
    "\n",
    "\n",
    "*   *embedding_dim*: The number of dimensions in the distributed representation.\n",
    "*   *num_hid*: The number of hidden units\n",
    "\n",
    "\n",
    "As the model trains, the script prints out some numbers that tell you how well the training is going.\n",
    "It shows:\n",
    "\n",
    "\n",
    "*   The cross entropy on the last 100 mini-batches of the training set. This is shown after every 100 mini-batches.\n",
    "*   The cross entropy on the entire validation set every 1000 mini-batches of training.\n",
    "\n",
    "At the end of training, this function shows the cross entropies on the training, validation and test sets.\n",
    "It will return a *Model* instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akBYJQOdLfaF"
   },
   "outputs": [],
   "source": [
    "_train_inputs = None\n",
    "_train_targets = None\n",
    "_vocab = None\n",
    "\n",
    "DEFAULT_TRAINING_CONFIG = {'batch_size': 100,  # the size of a mini-batch\n",
    "                           'learning_rate': 0.1,  # the learning rate\n",
    "                           'momentum': 0.9,  # the decay parameter for the momentum vector\n",
    "                           'epochs': 50,  # the maximum number of epochs to run\n",
    "                           'init_wt': 0.01,  # the standard deviation of the initial random weights\n",
    "                           'context_len': 3,  # the number of context words used\n",
    "                           'show_training_CE_after': 100,  # measure training error after this many mini-batches\n",
    "                           'show_validation_CE_after': 1000,  # measure validation error after this many mini-batches\n",
    "                           }\n",
    "\n",
    "\n",
    "def find_occurrences(word1, word2, word3):\n",
    "    \"\"\"Lists all the words that followed a given tri-gram in the training set and the number of\n",
    "    times each one followed it.\"\"\"\n",
    "\n",
    "    # cache the data so we don't keep reloading\n",
    "    global _train_inputs, _train_targets, _vocab\n",
    "    if _train_inputs is None:\n",
    "        data_obj = pickle.load(open(data_location, 'rb'))\n",
    "        _vocab = data_obj['vocab']\n",
    "        _train_inputs, _train_targets = data_obj['train_inputs'], data_obj['train_targets']\n",
    "\n",
    "    if word1 not in _vocab:\n",
    "        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n",
    "    if word2 not in _vocab:\n",
    "        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n",
    "    if word3 not in _vocab:\n",
    "        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word3))\n",
    "\n",
    "    idx1, idx2, idx3 = _vocab.index(word1), _vocab.index(word2), _vocab.index(word3)\n",
    "    idxs = np.array([idx1, idx2, idx3])\n",
    "\n",
    "    matches = np.all(_train_inputs == idxs.reshape((1, -1)), 1)\n",
    "\n",
    "    if np.any(matches):\n",
    "        counts = collections.defaultdict(int)\n",
    "        for m in np.where(matches)[0]:\n",
    "            counts[_vocab[_train_targets[m]]] += 1\n",
    "\n",
    "        word_counts = sorted(list(counts.items()), key=lambda t: t[1], reverse=True)\n",
    "        print('The tri-gram \"{} {} {}\" was followed by the following words in the training set:'.format(\n",
    "            word1, word2, word3))\n",
    "        for word, count in word_counts:\n",
    "            if count > 1:\n",
    "                print('    {} ({} times)'.format(word, count))\n",
    "            else:\n",
    "                print('    {} (1 time)'.format(word))\n",
    "    else:\n",
    "        print('The tri-gram \"{} {} {}\" did not occur in the training set.'.format(word1, word2, word3))\n",
    "\n",
    "\n",
    "def train(embedding_dim, num_hid, config=DEFAULT_TRAINING_CONFIG):\n",
    "    \"\"\"This is the main training routine for the language model. It takes two parameters:\n",
    "\n",
    "        embedding_dim, the dimension of the embeddilanguage_model.pyng space\n",
    "        num_hid, the number of hidden units.\"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    data_obj = pickle.load(open(data_location, 'rb'))\n",
    "    vocab = data_obj['vocab']\n",
    "    train_inputs, train_targets = data_obj['train_inputs'], data_obj['train_targets']\n",
    "    valid_inputs, valid_targets = data_obj['valid_inputs'], data_obj['valid_targets']\n",
    "    test_inputs, test_targets = data_obj['test_inputs'], data_obj['test_targets']\n",
    "\n",
    "    # Randomly initialize the trainable parameters\n",
    "    model = Model.random_init(config['init_wt'], vocab, config['context_len'], embedding_dim, num_hid)\n",
    "\n",
    "    # Variables used for early stopping\n",
    "    best_valid_CE = np.infty\n",
    "    end_training = False\n",
    "\n",
    "    # Initialize the momentum vector to all zeros\n",
    "    delta = Params.zeros(len(vocab), config['context_len'], embedding_dim, num_hid)\n",
    "\n",
    "    this_chunk_CE = 0.\n",
    "    batch_count = 0\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        if end_training:\n",
    "            break\n",
    "\n",
    "        print()\n",
    "        print('Epoch', epoch)\n",
    "\n",
    "        for m, (input_batch, target_batch) in enumerate(get_batches(train_inputs, train_targets,\n",
    "                                                                    config['batch_size'])):\n",
    "            batch_count += 1\n",
    "\n",
    "            # Forward propagate\n",
    "            activations = model.compute_activations(input_batch)\n",
    "\n",
    "            # Compute loss derivative\n",
    "            expanded_target_batch = model.indicator_matrix(target_batch)\n",
    "            loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch)\n",
    "            loss_derivative /= config['batch_size']\n",
    "\n",
    "            # Measure loss function\n",
    "            cross_entropy = model.compute_loss(activations.output_layer, expanded_target_batch) / config['batch_size']\n",
    "            this_chunk_CE += cross_entropy\n",
    "            if batch_count % config['show_training_CE_after'] == 0:\n",
    "                print('Batch {} Train CE {:1.3f}'.format(\n",
    "                    batch_count, this_chunk_CE / config['show_training_CE_after']))\n",
    "                this_chunk_CE = 0.\n",
    "\n",
    "            # Backpropagate\n",
    "            loss_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n",
    "\n",
    "            # Update the momentum vector and model parameters\n",
    "            delta = config['momentum'] * delta + loss_gradient\n",
    "            model.params -= config['learning_rate'] * delta\n",
    "\n",
    "            # Validate\n",
    "            if batch_count % config['show_validation_CE_after'] == 0:\n",
    "                print('Running validation...')\n",
    "                cross_entropy = model.evaluate(valid_inputs, valid_targets)\n",
    "                print('Validation cross-entropy: {:1.3f}'.format(cross_entropy))\n",
    "\n",
    "                if cross_entropy > best_valid_CE:\n",
    "                    print('Validation error increasing!  Training stopped.')\n",
    "                    end_training = True\n",
    "                    break\n",
    "\n",
    "                best_valid_CE = cross_entropy\n",
    "\n",
    "    print()\n",
    "    train_CE = model.evaluate(train_inputs, train_targets)\n",
    "    print('Final training cross-entropy: {:1.3f}'.format(train_CE))\n",
    "    valid_CE = model.evaluate(valid_inputs, valid_targets)\n",
    "    print('Final validation cross-entropy: {:1.3f}'.format(valid_CE))\n",
    "    test_CE = model.evaluate(test_inputs, test_targets)\n",
    "    print('Final test cross-entropy: {:1.3f}'.format(test_CE))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZX-g3K-F55h7"
   },
   "source": [
    "Run the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwlRG7j8LmIM"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "num_hid = 128\n",
    "trained_model = train(embedding_dim, num_hid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FD5Om0ypNPe"
   },
   "source": [
    "To convince us that you have correctly implemented the gradient computations, please include the following with your assignment submission:\n",
    "\n",
    "\n",
    "\n",
    "*   You will submit *language_model.ipynb* through MarkUs.\n",
    "You do not need to modify any of the code except the parts we asked you to implement.\n",
    "*   In your writeup, include the output of the function *print_gradients*.\n",
    "This prints out part of the gradients for a partially trained network which we have provided, and we will check them against the correct outputs. **Important:** make sure to give the output of *print_gradients*, not *check_gradients*.\n",
    "\n",
    "This is worth 4 points: 1 for the loss derivatives, 1 for the bias gradients, and 2 for the weight gradients.\n",
    "Since we gave you a gradient checker, you have no excuse for not getting full points on this part.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-TThqObiw58"
   },
   "source": [
    "# Part 4: Analysis (4pts)\n",
    "\n",
    "In this part, you will analyze the representation learned by the network. You should first train a\n",
    "model with a 16-dimensional embedding and 128 hidden units, as discussed in the previous section;\n",
    "youll use this trained model for the remainder of this section.\n",
    "Important: if youve made any fixes\n",
    "to your gradient code, you must reload the language_model module and then re-run the training\n",
    "procedure.\n",
    "Python does not reload modules automatically, and you dont want to accidentally\n",
    "analyze an old version of your model.\n",
    "\n",
    "These methods of the Model class can be used for analyzing the model after the training is\n",
    "done:\n",
    "\n",
    "\n",
    "\n",
    "*   *display_nearest_words* lists the words whose embedding vectors are nearest to the given\n",
    "word\n",
    "*   *word_distance* computes the distance between the embeddings of two words\n",
    "*   *predict_next_word* shows the possible next words the model considers most likely, along\n",
    "with their probabilities\n",
    "\n",
    "\n",
    "We also include:\n",
    "\n",
    "\n",
    "*    *tsne_plot_representation* creates a 2-dimensional embedding of the distributed representation space using\n",
    "an algorithm called t-SNE. (You dont need to know what this is for the assignment, but we\n",
    "may cover it later in the course.) Nearby points in this 2-D space are meant to correspond to\n",
    "nearby points in the 16-D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1PCshvD3ijGM"
   },
   "outputs": [],
   "source": [
    "import numpy as Math\n",
    "\n",
    "def Hbeta(D=Math.array([]), beta=1.0):\n",
    "    \"\"\"Compute the perplexity and the P-row for a specific value of the precision of a Gaussian distribution.\"\"\"\n",
    "\n",
    "    # Compute P-row and corresponding perplexity\n",
    "    P = Math.exp(-D.copy() * beta);\n",
    "    sumP = sum(P);\n",
    "    H = Math.log(sumP) + beta * Math.sum(D * P) / sumP;\n",
    "    P = P / sumP;\n",
    "    return H, P;\n",
    "\n",
    "\n",
    "def x2p(X=Math.array([]), tol=1e-5, perplexity=30.0):\n",
    "    \"\"\"Performs a binary search to get P-values in such a way that each conditional Gaussian has the same perplexity.\"\"\"\n",
    "\n",
    "    # Initialize some variables\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    (n, d) = X.shape;\n",
    "    sum_X = Math.sum(Math.square(X), 1);\n",
    "    D = Math.add(Math.add(-2 * Math.dot(X, X.T), sum_X).T, sum_X);\n",
    "    P = Math.zeros((n, n));\n",
    "    beta = Math.ones((n, 1));\n",
    "    logU = Math.log(perplexity);\n",
    "\n",
    "    # Loop over all datapoints\n",
    "    for i in range(n):\n",
    "\n",
    "        # Print progress\n",
    "        if i % 500 == 0:\n",
    "            print(\"Computing P-values for point \", i, \" of \", n, \"...\")\n",
    "\n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        betamin = -Math.inf;\n",
    "        betamax = Math.inf;\n",
    "        Di = D[i, Math.concatenate((Math.r_[0:i], Math.r_[i + 1:n]))];\n",
    "        (H, thisP) = Hbeta(Di, beta[i]);\n",
    "\n",
    "        # Evaluate whether the perplexity is within tolerance\n",
    "        Hdiff = H - logU;\n",
    "        tries = 0;\n",
    "        while Math.abs(Hdiff) > tol and tries < 50:\n",
    "\n",
    "            # If not, increase or decrease precision\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta[i];\n",
    "                if betamax == Math.inf or betamax == -Math.inf:\n",
    "                    beta[i] = beta[i] * 2;\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2;\n",
    "            else:\n",
    "                betamax = beta[i];\n",
    "                if betamin == Math.inf or betamin == -Math.inf:\n",
    "                    beta[i] = beta[i] / 2;\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2;\n",
    "\n",
    "            # Recompute the values\n",
    "            (H, thisP) = Hbeta(Di, beta[i]);\n",
    "            Hdiff = H - logU;\n",
    "            tries = tries + 1;\n",
    "\n",
    "        # Set the final row of P\n",
    "        P[i, Math.concatenate((Math.r_[0:i], Math.r_[i + 1:n]))] = thisP;\n",
    "\n",
    "    # Return final P-matrix\n",
    "    print(\"Mean value of sigma: \", Math.mean(Math.sqrt(1 / beta)))\n",
    "    return P;\n",
    "\n",
    "\n",
    "def pca(X=Math.array([]), no_dims=50):\n",
    "    \"\"\"Runs PCA on the NxD array X in order to reduce its dimensionality to no_dims dimensions.\"\"\"\n",
    "\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    (n, d) = X.shape;\n",
    "    X = X - Math.tile(Math.mean(X, 0), (n, 1));\n",
    "    (l, M) = Math.linalg.eig(Math.dot(X.T, X));\n",
    "    Y = Math.dot(X, M[:, 0:no_dims]);\n",
    "    return Y;\n",
    "\n",
    "\n",
    "def tsne(X=Math.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\n",
    "    \"\"\"Runs t-SNE on the dataset in the NxD array X to reduce its dimensionality to no_dims dimensions.\n",
    "    The syntaxis of the function is Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\"\"\"\n",
    "\n",
    "    # Check inputs\n",
    "    if X.dtype != \"float64\":\n",
    "        print(\"Error: array X should have type float64.\");\n",
    "        return -1;\n",
    "    # if no_dims.__class__ != \"<type 'int'>\":\t\t\t# doesn't work yet!\n",
    "    #\tprint(\"Error: number of dimensions should be an integer.\");\n",
    "    #\treturn -1;\n",
    "\n",
    "    # Initialize variables\n",
    "    X = pca(X, initial_dims);\n",
    "    (n, d) = X.shape;\n",
    "    max_iter = 1000;\n",
    "    initial_momentum = 0.5;\n",
    "    final_momentum = 0.8;\n",
    "    eta = 500;\n",
    "    min_gain = 0.01;\n",
    "    Y = Math.random.randn(n, no_dims);\n",
    "    dY = Math.zeros((n, no_dims));\n",
    "    iY = Math.zeros((n, no_dims));\n",
    "    gains = Math.ones((n, no_dims));\n",
    "\n",
    "    # Compute P-values\n",
    "    P = x2p(X, 1e-5, perplexity);\n",
    "    P = P + Math.transpose(P);\n",
    "    P = P / Math.sum(P);\n",
    "    P = P * 4;  # early exaggeration\n",
    "    P = Math.maximum(P, 1e-12);\n",
    "\n",
    "    # Run iterations\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        # Compute pairwise affinities\n",
    "        sum_Y = Math.sum(Math.square(Y), 1);\n",
    "        num = 1 / (1 + Math.add(Math.add(-2 * Math.dot(Y, Y.T), sum_Y).T, sum_Y));\n",
    "        num[range(n), range(n)] = 0;\n",
    "        Q = num / Math.sum(num);\n",
    "        Q = Math.maximum(Q, 1e-12);\n",
    "\n",
    "        # Compute gradient\n",
    "        PQ = P - Q;\n",
    "        for i in range(n):\n",
    "            dY[i, :] = Math.sum(Math.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0);\n",
    "\n",
    "        # Perform the update\n",
    "        if iter < 20:\n",
    "            momentum = initial_momentum\n",
    "        else:\n",
    "            momentum = final_momentum\n",
    "        gains = (gains + 0.2) * ((dY > 0) != (iY > 0)) + (gains * 0.8) * ((dY > 0) == (iY > 0));\n",
    "        gains[gains < min_gain] = min_gain;\n",
    "        iY = momentum * iY - eta * (gains * dY);\n",
    "        Y = Y + iY;\n",
    "        Y = Y - Math.tile(Math.mean(Y, 0), (n, 1));\n",
    "\n",
    "        # Compute current value of cost function\n",
    "        if (iter + 1) % 10 == 0:\n",
    "            C = Math.sum(P * Math.log(P / Q));\n",
    "            print(\"Iteration \", (iter + 1), \": error is \", C)\n",
    "\n",
    "        # Stop lying about P-values\n",
    "        if iter == 100:\n",
    "            P = P / 4;\n",
    "\n",
    "    # Return solution\n",
    "    return Y;\n",
    "\n",
    "def tsne_plot_representation(model):\n",
    "    \"\"\"Plot a 2-D visualization of the learned representations using t-SNE.\"\"\"\n",
    "    print(model.params.word_embedding_weights.shape)\n",
    "    mapped_X = tsne(model.params.word_embedding_weights)\n",
    "    pylab.figure()\n",
    "    for i, w in enumerate(model.vocab):\n",
    "        pylab.text(mapped_X[i, 0], mapped_X[i, 1], w)\n",
    "    pylab.xlim(mapped_X[:, 0].min(), mapped_X[:, 0].max())\n",
    "    pylab.ylim(mapped_X[:, 1].min(), mapped_X[:, 1].max())\n",
    "    pylab.show()\n",
    "\n",
    "def tsne_plot_GLoVE_representation(W_final, b_final):\n",
    "    \"\"\"Plot a 2-D visualization of the learned representations using t-SNE.\"\"\"\n",
    "    mapped_X = tsne(W_final)\n",
    "    pylab.figure()\n",
    "    data_obj = pickle.load(open(data_location, 'rb'))\n",
    "    for i, w in enumerate(data_obj['vocab']):\n",
    "        pylab.text(mapped_X[i, 0], mapped_X[i, 1], w)\n",
    "    pylab.xlim(mapped_X[:, 0].min(), mapped_X[:, 0].max())\n",
    "    pylab.ylim(mapped_X[:, 1].min(), mapped_X[:, 1].max())\n",
    "    pylab.show()\n",
    "\n",
    "def plot_2d_GLoVE_representation(W_final, b_final):\n",
    "    \"\"\"Plot a 2-D visualization of the learned representations.\"\"\"\n",
    "    mapped_X = W_final\n",
    "    pylab.figure()\n",
    "    data_obj = pickle.load(open(data_location, 'rb'))\n",
    "    for i, w in enumerate(data_obj['vocab']):\n",
    "        pylab.text(mapped_X[i, 0], mapped_X[i, 1], w)\n",
    "    pylab.xlim(mapped_X[:, 0].min(), mapped_X[:, 0].max())\n",
    "    pylab.ylim(mapped_X[:, 1].min(), mapped_X[:, 1].max())\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ZokGoNbivHp"
   },
   "source": [
    "Using these methods, please answer the following questions, each of which is worth 1 point.\n",
    "\n",
    "\n",
    "\n",
    "1.   Pick three words from the vocabulary that go well together (for example, *government of united*,\n",
    "*city of new*, *life in the*, *he is the* etc.).\n",
    "Use the model to predict the next word.\n",
    "Does the model give sensible predictions?\n",
    "Try to find an example where it makes a plausible prediction even though the 4-gram wasnt present in the dataset (*raw_sentences.txt*).\n",
    "To help you out, the function *find_occurrences* lists the words that appear after a given 3-gram in the training set.\n",
    "2.   Plot the 2-dimensional visualization using the method *tsne_plot_representation*.\n",
    "Look at the plot and find a few clusters of related words.\n",
    "What do the words in each cluster have in common?\n",
    "Plot the 2-dimensional visualization using the method *tsne_plot_GLoVE_representation* for a 256 dimensional embedding.\n",
    "How do the t-SNE embeddings for both models compare?\n",
    "Plot the 2-dimensional visualization using the method *plot_2d_GLoVE_representation*.\n",
    "How does this compare to the t-SNE embeddings?\n",
    "(You dont need to include the plots with your submission.)\n",
    "\n",
    "3.   Are the words *new* and *york* close together in the learned representation?\n",
    "Why or why not?\n",
    "4.   Which pair of words is closer together in the learned representation: (*government*, *political*), or (*government*, *university*)?\n",
    "Why do you think this is?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZjEVB1oirWq"
   },
   "outputs": [],
   "source": [
    "tsne_plot_representation(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gJQE9Gxsftn"
   },
   "outputs": [],
   "source": [
    "tsne_plot_GLoVE_representation(W_final, b_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pw5WKPyntY_O"
   },
   "outputs": [],
   "source": [
    "plot_2d_GLoVE_representation(W_final_2d, b_final_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MR6DEZvM_0jg"
   },
   "outputs": [],
   "source": [
    "tsne_plot_GLoVE_representation(W_final_2d, b_final_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3RzXTTzsdy9"
   },
   "source": [
    "# What you have to submit\n",
    "\n",
    "For reference, here is everything you need to hand in. See the top of this handout for submission\n",
    "directions.\n",
    "\n",
    "\n",
    "\n",
    "*   A PDF file titled *a1-writeup.pdf* containing the following:\n",
    "Both questions from Part 2, the output of *print_gradients()*, answers to all four questions from Part 3.\n",
    "*   Your code file *language_model.ipynb*\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "language_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
